<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Optimizing Matrix Multiplication with Zig | Svanik Sharma&#39;s Website</title>
<meta name="keywords" content="">
<meta name="description" content="I recently started playing with the Zig programming language and wanted to try it out for its speed. And what better way to do that than to try optimizing matrix multiplication? Since there are a plethora of resources to understand how to multiply matrices efficiently (see the Resources section below), I won&rsquo;t be doing anything intense in this article (though maybe in the future I will).
The naive matrix multiplication algorithm is given below in Zig:">
<meta name="author" content="">
<link rel="canonical" href="https://svaniksharma.github.io/posts/2023-05-07-optimizing-matrix-multiplication-with-zig/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://svaniksharma.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://svaniksharma.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://svaniksharma.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://svaniksharma.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://svaniksharma.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://svaniksharma.github.io/posts/2023-05-07-optimizing-matrix-multiplication-with-zig/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

  

<meta property="og:title" content="Optimizing Matrix Multiplication with Zig" />
<meta property="og:description" content="I recently started playing with the Zig programming language and wanted to try it out for its speed. And what better way to do that than to try optimizing matrix multiplication? Since there are a plethora of resources to understand how to multiply matrices efficiently (see the Resources section below), I won&rsquo;t be doing anything intense in this article (though maybe in the future I will).
The naive matrix multiplication algorithm is given below in Zig:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://svaniksharma.github.io/posts/2023-05-07-optimizing-matrix-multiplication-with-zig/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-07T07:07:07+01:00" />
<meta property="article:modified_time" content="2023-05-07T07:07:07+01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Optimizing Matrix Multiplication with Zig"/>
<meta name="twitter:description" content="I recently started playing with the Zig programming language and wanted to try it out for its speed. And what better way to do that than to try optimizing matrix multiplication? Since there are a plethora of resources to understand how to multiply matrices efficiently (see the Resources section below), I won&rsquo;t be doing anything intense in this article (though maybe in the future I will).
The naive matrix multiplication algorithm is given below in Zig:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://svaniksharma.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Optimizing Matrix Multiplication with Zig",
      "item": "https://svaniksharma.github.io/posts/2023-05-07-optimizing-matrix-multiplication-with-zig/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Optimizing Matrix Multiplication with Zig",
  "name": "Optimizing Matrix Multiplication with Zig",
  "description": "I recently started playing with the Zig programming language and wanted to try it out for its speed. And what better way to do that than to try optimizing matrix multiplication? Since there are a plethora of resources to understand how to multiply matrices efficiently (see the Resources section below), I won\u0026rsquo;t be doing anything intense in this article (though maybe in the future I will).\nThe naive matrix multiplication algorithm is given below in Zig:",
  "keywords": [
    
  ],
  "articleBody": "I recently started playing with the Zig programming language and wanted to try it out for its speed. And what better way to do that than to try optimizing matrix multiplication? Since there are a plethora of resources to understand how to multiply matrices efficiently (see the Resources section below), I won’t be doing anything intense in this article (though maybe in the future I will).\nThe naive matrix multiplication algorithm is given below in Zig:\nfn naiveMatrixMultiply(C: anytype, A: anytype, B: anytype) void { const N = A.len; for (0..N) |i| { for (0..N) |j| { for (0..N) |k| { C[i][j] += A[i][k] * B[k][j]; } } } } We’ll iteratively optimize this, using perf to benchmark our findings. At the bottom of this post is the boiler plate I used to test the code’s correctness, how I generated the matrices, etc. The optimizations done here are applied to square matrices and all perf output shows the time taken to multiply two randomly-generated 1000 x 1000 matrices. The following is the output of perf stat -e cache-misses,cache-references,instructions,cycles ./matrix after compiling zig build-exe matrix.zig and running naiveMatrixMultiply:\n1.5698057476e+04 ms Performance counter stats for './matrix': 127,513,715 cache-misses # 96.317 % of all cache refs 132,389,208 cache-references 87,573,830,478 instructions # 1.91 insn per cycle 45,758,041,155 cycles 15.928915857 seconds time elapsed 15.899428000 seconds user 0.023993000 seconds sys In the original code, I measure the time taken to execute the function and then print it out, which is why there is a 1.56e+04 ms at the top of the output. We won’t be using this for benchmarking, however. Unless otherwise stated, all perf output is the result of running zig build-exe matrix.zig and then running perf stat -e cache-misses,cache-references,instructions,cycles ./matrix.\nOptimization #1: Transpose the matrix Notice that matrix B is iterated over in column-major order. That is, we iterate over the elements of B like so: (0, 0), (1, 0), (2, 0), …, (N-1, 0), (0, 1), etc. Notice that (0, 0) and (0, 1) are in the same cache line. Therefore, by transposing B, we can ensure that we are traversing both matrices in row-major order, which allows us to hit the cache more often.\nfn transposeMatrixMultiply(C: anytype, A: anytype, B: anytype) !void { var arena = std.heap.ArenaAllocator.init(std.heap.page_allocator); defer arena.deinit(); const allocator = arena.allocator(); var tmp: [][]f64 = try allocator.alloc([]f64, B.len); for (tmp) |*row| { row.* = try allocator.alloc(f64, B.len); } for (0..B.len) |i| { for (0..B.len) |j| { tmp[i][j] = B[j][i]; } } for (0..B.len) |i| { for (0..B.len) |j| { for (0..B.len) |k| { C[i][j] += A[i][k] * tmp[j][k]; } } } } Our perf stat output for this new function is:\n1.0450551452e+04 ms Performance counter stats for './matrix': 4,606,282 cache-misses # 65.597 % of all cache refs 7,022,066 cache-references 91,634,171,605 instructions # 2.99 insn per cycle 30,664,602,948 cycles 10.680824428 seconds time elapsed 10.665631000 seconds user 0.011997000 seconds sys Compared to the naive output, this is pretty good. We are definitely hitting the cache more often and, in terms, of cycles, we see a 33% speedup! However, we can do even better.\nOptimization 2: SIMD + Transpose In addition to the transpose, we can try to use SIMD (Single-Instruction, Multiple-Data) instructions. If we were programming this in C, we would have to use SIMD intrinsics, which are not only sometimes difficult to use but not very portable. However, Zig offers the Vector datatype, which allows one to operate on multiple elements at the same time. The code now looks like this:\nfn transposeSimdMatrixMultiply(C: anytype, A: anytype, B: anytype) !void { var arena = std.heap.ArenaAllocator.init(std.heap.page_allocator); defer arena.deinit(); const allocator = arena.allocator(); var tmp: [][]f64 = try allocator.alloc([]f64, B.len); for (tmp) |*row| { row.* = try allocator.alloc(f64, B.len); } for (0..B.len) |i| { for (0..B.len) |j| { tmp[i][j] = B[j][i]; } } const vec_len = 32; for (0..B.len) |i| { for (0..B.len) |j| { var k: usize = 0; while (k \u003c= B.len - vec_len) : (k += vec_len) { const u: @Vector(vec_len, f64) = A[i][k..][0..vec_len].*; const v: @Vector(vec_len, f64) = tmp[j][k..][0..vec_len].*; C[i][j] += @reduce(.Add, u * v); } while (k \u003c B.len) : (k += 1) { C[i][j] += A[i][k] * tmp[j][k]; } } } } It’s a bit longer, but there’s nothing too bad here. The innermost loop now operates on vec_len = 32 elements at a time, multiplying sets of 32 elements in each row of A and tmp and then summing the elementwise products together. If the number of elements left at the end of the loop isn’t a multiple of 32, then we revert back to the same algorithm as the transposeMatrixMultiply function. Here’s the perf output:\n2.319826094e+03 ms Performance counter stats for './matrix': 5,522,065 cache-misses # 21.862 % of all cache refs 25,258,801 cache-references 12,249,595,729 instructions # 1.84 insn per cycle 6,672,691,818 cycles 2.546859528 seconds time elapsed 2.538763000 seconds user 0.008008000 seconds sys Again, a substantial decrease. We’re now operating at 6 billion cycles, or 14% of the number of cycles taken by the naive function. Also, a much smaller proportion of our cache references are cache misses. Compared to working with SIMD intrinsics by hand, this definitely is a lot of power at your fingertips. In the AVX2 instruction set (which is the instruction set my machine uses), each vector register is 256 bits and there are 8 registers, but if you are using intrinsics, you would have to manage these 8 registers separately. However, the generic interface provided by Vector means that we can treat these 8 registers as one big register, each containing 4 64-bit floating point numbers, allowing us to operate on 32 elements at a time!\nOptimization 3: SIMD with unrolled loop Practically, we cannot always afford to transpose a matrix. Besides the O(N^2) runtime to actually transpose it, we also take up extra memory. For the next optimization, we will use SIMD, but we won’t transpose the B matrix. Going back to what we observed before, the access pattern for B is suboptimal: when we access B[k][j], we access B[k][j+1] in the inner loop only after another round of the inner loop. However, that if we did use it while we were in the inner loop. That is, while we computed C[i][j] += A[i][k] * B[k][j], we also computed C[i][j+1] += A[i][k] * B[k][j+1]? Since i doesn’t change until the j-loop is over and k only changes after we are done with C[i][j] += A[i][k] * B[k][j], we can take advantage of the moment we have access to the elements in the slice B[k][j..] and use it to compute the elements that will belong in C[i][j..]. The following code puts this thought into action:\nfn unrollSimdMatrixMultiply(C: anytype, A: anytype, B: anytype) void { const N = B.len; const vec_len = 32; for (C, A) |*C_row, *A_row| { var j: u32 = 0; while (j \u003c= N - vec_len) : (j += vec_len) { for (0..N) |k| { const u: @Vector(vec_len, f64) = B[k][j..][0..vec_len].*; const y: @Vector(vec_len, f64) = C_row.*[j..][0..vec_len].*; const w: @Vector(vec_len, f64) = @splat(vec_len, A_row.*[k]); const slice: [vec_len]f64 = (u * w) + y; @memcpy(C_row.*[j .. j + vec_len], \u0026slice); } } while (j \u003c N) : (j += 1) { for (0..N) |k| { C_row.*[j] += A_row.*[k] * B[k][j]; } } } } Note that I replaced the i loop and decided to loop over the rows of C and A directly. What we are doing is again straightforward. We just take vec_len = 32 elements B[k][j], B[k][j+1], ..., B[k][j + 31], multiply them by A[i][k] (which is now A_row.*[k]), and then store it in C[i][j], C[i][j+1], ..., C[i][j + 31] (which is now C_row.*[j], C_row.*[j+1], ..., C_row.*[j+31]). Again, if we have less than 32 elements remaining, we revert back to the standard multiplication algorithm. As always, the perf output is below:\n5.233718283e+03 ms Performance counter stats for './matrix': 101,785,707 cache-misses # 63.052 % of all cache refs 161,432,535 cache-references 16,377,067,907 instructions # 1.15 insn per cycle 14,227,983,666 cycles 5.462324961 seconds time elapsed 5.457798000 seconds user 0.004001000 seconds sys Compared to our previous output, this isn’t great. However, still a significant improvement from our naive and only-transpose matrix multiplication functions. The main issue here is the k-loop: though we are leveraging Vector to use nearby data, we are still missing the cache (and in fact our proportion of cache-misses in relation to cache-references is similar to the only-transpose matrix function). Still we are not using up extra memory, which is a good bonus. However, there is one more optimization we can do.\nOptimization 4: Compilation Arguments By default, Zig is in the Debug build mode, which means that it enables all runtime safety checks with no optimizations. However, we can change this build mode by running zig build-exe -O ReleaseFast ./matrix (which builds without runtime-safety checks and optimizes for speed). Now running perf on the unrolled SIMD loop, we get the following:\n1.636596222e+03 ms Performance counter stats for './matrix': 116,986,587 cache-misses # 70.848 % of all cache refs 165,123,281 cache-references 1,133,618,990 instructions # 0.29 insn per cycle 3,970,347,473 cycles 1.652725296 seconds time elapsed 1.640017000 seconds user 0.012000000 seconds sys With quite literally zero coding effort or thinking at all, we have beaten our previous record! To be fair, if you run zig build-exe -O ReleaseFast ./matrix using the transposeSimdMatrixMultiply function, you will find that it is still faster with optimizations as well. However, considering we were trying to avoid transposes and put in minimal effort, I would say this level of optimization is pretty good. Another thing I should note is that a significant proportion of the time is taken with the builtin @memcpy function. Running perf record -a ./matrix in Debug mode and then looking at perf report gives me this output:\n81.08% matrix matrix [.] matrix.unrollSimdMatrixMultiply__anon_3611 9.86% matrix matrix [.] memcpy 1.46% swapper [unknown] [k] 0xffffffffb372cdaf 1.42% matrix matrix [.] rand.Xoshiro256.fill ... rest of output omitted I also tried using std.mem.copy, but it was actually worse than the builtin function. However, the reason I used @memcpy was because there doesn’t seem to be another choice. There was no “store” function I could use. If we were in C, we could have just used something like _mm256_store_pd to store the data into C, but the Vector datatype does not seem to have anything like that. However, I think the Vector interface is still being worked on, so it’s possible this will be ironed out in later versions.\nConclusion In the resources section below, I provided some links to some good material I found on matrix multiplication and how to optimize it. If you look at them, you’ll find that it can get quite involved really quickly. However, most of the optimizations are related to improving cache hits. Furthermore, what I did in this article is by no means the limit, and especially not with Zig. I’m sure if you dug into the @memcpy function, figured out how to use SIMD intrinsics within Zig, or used some of the other builtin functions (like @preFetch, which sounds quite useful in this case), you can further optimize what I wrote. Plus, I’m a complete beginner to Zig, so I’m pretty sure a lot of what I wrote was suboptimal to some degree. Nevertheless, I’m quite optimistic to see a performance-oriented language like this, and being able to optimize a complex problem like this very quickly is extremely promising. The full code is available here.\nResources What Every Programmer Should Know About Memory Matrix Multiplication Matrix multiplication algorithm ",
  "wordCount" : "1906",
  "inLanguage": "en",
  "datePublished": "2023-05-07T07:07:07+01:00",
  "dateModified": "2023-05-07T07:07:07+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://svaniksharma.github.io/posts/2023-05-07-optimizing-matrix-multiplication-with-zig/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Svanik Sharma's Website",
    "logo": {
      "@type": "ImageObject",
      "url": "https://svaniksharma.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://svaniksharma.github.io/" accesskey="h" title="Svanik Sharma&#39;s Website (Alt + H)">Svanik Sharma&#39;s Website</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Optimizing Matrix Multiplication with Zig
    </h1>
    <div class="post-meta"><span title='2023-05-07 07:07:07 +0100 +0100'>May 7, 2023</span>

</div>
  </header> 
  <div class="post-content"><p>I recently started playing with the <a href="https://ziglang.org/">Zig</a> programming language and wanted to try it out for its speed.
And what better way to do that than to try optimizing matrix multiplication? Since there are a plethora of resources to
understand how to multiply matrices efficiently (see the Resources section below), I won&rsquo;t be doing anything intense in this article
(though maybe in the future I will).</p>
<p>The naive matrix multiplication algorithm is given below in Zig:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-zig" data-lang="zig"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> naiveMatrixMultiply(C<span style="color:#f92672">:</span> anytype, A<span style="color:#f92672">:</span> anytype, B<span style="color:#f92672">:</span> anytype) <span style="color:#66d9ef">void</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> N <span style="color:#f92672">=</span> A.len;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..N) <span style="color:#f92672">|</span>i<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..N) <span style="color:#f92672">|</span>j<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..N) <span style="color:#f92672">|</span>k<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>                C[i][j] <span style="color:#f92672">+=</span> A[i][k] <span style="color:#f92672">*</span> B[k][j];
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>We&rsquo;ll iteratively optimize this, using <code>perf</code> to benchmark our findings. At the bottom of this post is the boiler plate I used to test the code&rsquo;s
correctness, how I generated the matrices, etc. The optimizations done here are applied to square matrices and all <code>perf</code> output shows the time
taken to multiply two randomly-generated 1000 x 1000 matrices. The following is the output of <code>perf stat -e cache-misses,cache-references,instructions,cycles ./matrix</code> after compiling <code>zig build-exe matrix.zig</code>
and running <code>naiveMatrixMultiply</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>1.5698057476e+04 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> Performance counter stats <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;./matrix&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       127,513,715      cache-misses              <span style="color:#75715e">#   96.317 % of all cache refs</span>
</span></span><span style="display:flex;"><span>       132,389,208      cache-references
</span></span><span style="display:flex;"><span>    87,573,830,478      instructions              <span style="color:#75715e">#    1.91  insn per cycle</span>
</span></span><span style="display:flex;"><span>    45,758,041,155      cycles
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      15.928915857 seconds time elapsed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      15.899428000 seconds user
</span></span><span style="display:flex;"><span>       0.023993000 seconds sys
</span></span></code></pre></div><p>In the original code, I measure the time taken to execute the function and then print it out, which is why there is a <code>1.56e+04 ms</code> at the top of the output. We won&rsquo;t be using this for benchmarking, however.
Unless otherwise stated, all <code>perf</code> output is the result of running <code>zig build-exe matrix.zig</code> and then running <code>perf stat -e cache-misses,cache-references,instructions,cycles ./matrix</code>.</p>
<h2 id="optimization-1-transpose-the-matrix">Optimization #1: Transpose the matrix<a hidden class="anchor" aria-hidden="true" href="#optimization-1-transpose-the-matrix">#</a></h2>
<p>Notice that matrix <code>B</code> is iterated over in column-major order. That is, we iterate over the elements of <code>B</code> like so: (0, 0), (1, 0), (2, 0), &hellip;, (N-1, 0), (0, 1), etc.
Notice that (0, 0) and (0, 1) are in the same cache line. Therefore, by transposing <code>B</code>, we can ensure that we are traversing both matrices in row-major order, which allows us to hit the cache more often.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-zig" data-lang="zig"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> transposeMatrixMultiply(C<span style="color:#f92672">:</span> anytype, A<span style="color:#f92672">:</span> anytype, B<span style="color:#f92672">:</span> anytype) <span style="color:#f92672">!</span><span style="color:#66d9ef">void</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">var</span> arena <span style="color:#f92672">=</span> std.heap.ArenaAllocator.init(std.heap.page_allocator);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">defer</span> arena.deinit();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> allocator <span style="color:#f92672">=</span> arena.allocator();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">var</span> tmp<span style="color:#f92672">:</span> [][]<span style="color:#66d9ef">f64</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">try</span> allocator.alloc([]<span style="color:#66d9ef">f64</span>, B.len);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (tmp) <span style="color:#f92672">|*</span>row<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        row.<span style="color:#f92672">*</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">try</span> allocator.alloc(<span style="color:#66d9ef">f64</span>, B.len);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..B.len) <span style="color:#f92672">|</span>i<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..B.len) <span style="color:#f92672">|</span>j<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>            tmp[i][j] <span style="color:#f92672">=</span> B[j][i];
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..B.len) <span style="color:#f92672">|</span>i<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..B.len) <span style="color:#f92672">|</span>j<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..B.len) <span style="color:#f92672">|</span>k<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>                C[i][j] <span style="color:#f92672">+=</span> A[i][k] <span style="color:#f92672">*</span> tmp[j][k];
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Our <code>perf stat</code> output for this new function is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>1.0450551452e+04 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> Performance counter stats <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;./matrix&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>         4,606,282      cache-misses              <span style="color:#75715e">#   65.597 % of all cache refs</span>
</span></span><span style="display:flex;"><span>         7,022,066      cache-references
</span></span><span style="display:flex;"><span>    91,634,171,605      instructions              <span style="color:#75715e">#    2.99  insn per cycle</span>
</span></span><span style="display:flex;"><span>    30,664,602,948      cycles
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      10.680824428 seconds time elapsed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      10.665631000 seconds user
</span></span><span style="display:flex;"><span>       0.011997000 seconds sys
</span></span></code></pre></div><p>Compared to the naive output, this is pretty good. We are definitely hitting the cache more often and, in terms, of cycles,
we see a 33% speedup! However, we can do even better.</p>
<h2 id="optimization-2-simd--transpose">Optimization 2: SIMD + Transpose<a hidden class="anchor" aria-hidden="true" href="#optimization-2-simd--transpose">#</a></h2>
<p>In addition to the transpose, we can try to use SIMD (Single-Instruction, Multiple-Data) instructions. If we were programming this in C,
we would have to use SIMD intrinsics, which are not only sometimes difficult to use but not very portable. However, Zig offers
the <code>Vector</code> datatype, which allows one to operate on multiple elements at the same time. The code now looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>fn transposeSimdMatrixMultiply<span style="color:#f92672">(</span>C: anytype, A: anytype, B: anytype<span style="color:#f92672">)</span> !void <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>    var arena <span style="color:#f92672">=</span> std.heap.ArenaAllocator.init<span style="color:#f92672">(</span>std.heap.page_allocator<span style="color:#f92672">)</span>;
</span></span><span style="display:flex;"><span>    defer arena.deinit<span style="color:#f92672">()</span>;
</span></span><span style="display:flex;"><span>    const allocator <span style="color:#f92672">=</span> arena.allocator<span style="color:#f92672">()</span>;
</span></span><span style="display:flex;"><span>    var tmp: <span style="color:#f92672">[][]</span>f64 <span style="color:#f92672">=</span> try allocator.alloc<span style="color:#f92672">([]</span>f64, B.len<span style="color:#f92672">)</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span>tmp<span style="color:#f92672">)</span> |*row| <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>        row.* <span style="color:#f92672">=</span> try allocator.alloc<span style="color:#f92672">(</span>f64, B.len<span style="color:#f92672">)</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span>0..B.len<span style="color:#f92672">)</span> |i| <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span>0..B.len<span style="color:#f92672">)</span> |j| <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>            tmp<span style="color:#f92672">[</span>i<span style="color:#f92672">][</span>j<span style="color:#f92672">]</span> <span style="color:#f92672">=</span> B<span style="color:#f92672">[</span>j<span style="color:#f92672">][</span>i<span style="color:#f92672">]</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>    const vec_len <span style="color:#f92672">=</span> 32;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span>0..B.len<span style="color:#f92672">)</span> |i| <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span>0..B.len<span style="color:#f92672">)</span> |j| <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>            var k: usize <span style="color:#f92672">=</span> 0;
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">while</span> <span style="color:#f92672">(</span>k &lt;<span style="color:#f92672">=</span> B.len - vec_len<span style="color:#f92672">)</span> : <span style="color:#f92672">(</span>k <span style="color:#f92672">+=</span> vec_len<span style="color:#f92672">)</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>                const u: @Vector<span style="color:#f92672">(</span>vec_len, f64<span style="color:#f92672">)</span> <span style="color:#f92672">=</span> A<span style="color:#f92672">[</span>i<span style="color:#f92672">][</span>k..<span style="color:#f92672">][</span>0..vec_len<span style="color:#f92672">]</span>.*;
</span></span><span style="display:flex;"><span>                const v: @Vector<span style="color:#f92672">(</span>vec_len, f64<span style="color:#f92672">)</span> <span style="color:#f92672">=</span> tmp<span style="color:#f92672">[</span>j<span style="color:#f92672">][</span>k..<span style="color:#f92672">][</span>0..vec_len<span style="color:#f92672">]</span>.*;
</span></span><span style="display:flex;"><span>                C<span style="color:#f92672">[</span>i<span style="color:#f92672">][</span>j<span style="color:#f92672">]</span> +<span style="color:#f92672">=</span> @reduce<span style="color:#f92672">(</span>.Add, u * v<span style="color:#f92672">)</span>;
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">while</span> <span style="color:#f92672">(</span>k &lt; B.len<span style="color:#f92672">)</span> : <span style="color:#f92672">(</span>k <span style="color:#f92672">+=</span> 1<span style="color:#f92672">)</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>                C<span style="color:#f92672">[</span>i<span style="color:#f92672">][</span>j<span style="color:#f92672">]</span> +<span style="color:#f92672">=</span> A<span style="color:#f92672">[</span>i<span style="color:#f92672">][</span>k<span style="color:#f92672">]</span> * tmp<span style="color:#f92672">[</span>j<span style="color:#f92672">][</span>k<span style="color:#f92672">]</span>;
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>It&rsquo;s a bit longer, but there&rsquo;s nothing too bad here. The innermost loop now operates on <code>vec_len = 32</code> elements at a time,
multiplying sets of 32 elements in each row of <code>A</code> and <code>tmp</code> and then summing the elementwise products together. If the number
of elements left at the end of the loop isn&rsquo;t a multiple of 32, then we revert back to the same algorithm as the <code>transposeMatrixMultiply</code> function.
Here&rsquo;s the <code>perf</code> output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>2.319826094e+03 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> Performance counter stats <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;./matrix&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>         5,522,065      cache-misses              <span style="color:#75715e">#   21.862 % of all cache refs</span>
</span></span><span style="display:flex;"><span>        25,258,801      cache-references
</span></span><span style="display:flex;"><span>    12,249,595,729      instructions              <span style="color:#75715e">#    1.84  insn per cycle</span>
</span></span><span style="display:flex;"><span>     6,672,691,818      cycles
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       2.546859528 seconds time elapsed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       2.538763000 seconds user
</span></span><span style="display:flex;"><span>       0.008008000 seconds sys
</span></span></code></pre></div><p>Again, a substantial decrease. We&rsquo;re now operating at 6 billion cycles, or 14% of the number of cycles taken by the naive function.
Also, a much smaller proportion of our cache references are cache misses. Compared to working with SIMD intrinsics by hand, this
definitely is a lot of power at your fingertips. In the <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX2 instruction set</a> (which is
the instruction set my machine uses), each vector register is 256 bits and there are 8 registers, but if you are using intrinsics, you
would have to manage these 8 registers separately. However, the generic interface provided by <code>Vector</code> means that we can treat these 8
registers as one big register, each containing 4 64-bit floating point numbers, allowing us to operate on 32 elements at a time!</p>
<h2 id="optimization-3-simd-with-unrolled-loop">Optimization 3: SIMD with unrolled loop<a hidden class="anchor" aria-hidden="true" href="#optimization-3-simd-with-unrolled-loop">#</a></h2>
<p>Practically, we cannot always afford to transpose a matrix. Besides the <code>O(N^2)</code> runtime to actually transpose it, we also take up extra memory.
For the next optimization, we will use SIMD, but we won&rsquo;t transpose the <code>B</code> matrix. Going back to what we observed before, the access pattern
for <code>B</code> is suboptimal: when we access <code>B[k][j]</code>, we access <code>B[k][j+1]</code> in the inner loop only after <em>another</em> round of the inner loop. However,
that if we did use it while we were in the inner loop. That is, while we computed <code>C[i][j] += A[i][k] * B[k][j]</code>, we also computed <code>C[i][j+1] +=  A[i][k] * B[k][j+1]</code>? Since <code>i</code> doesn&rsquo;t change until the <code>j</code>-loop is over and <code>k</code> only changes after we are done with <code>C[i][j] += A[i][k] * B[k][j]</code>,
we can take advantage of the moment we have access to the elements in the slice <code>B[k][j..]</code> and use it to compute the elements that will belong in
<code>C[i][j..]</code>. The following code puts this thought into action:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-zig" data-lang="zig"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> unrollSimdMatrixMultiply(C<span style="color:#f92672">:</span> anytype, A<span style="color:#f92672">:</span> anytype, B<span style="color:#f92672">:</span> anytype) <span style="color:#66d9ef">void</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> N <span style="color:#f92672">=</span> B.len;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> vec_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (C, A) <span style="color:#f92672">|*</span>C_row, <span style="color:#f92672">*</span>A_row<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">var</span> j<span style="color:#f92672">:</span> <span style="color:#66d9ef">u32</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> (j <span style="color:#f92672">&lt;=</span> N <span style="color:#f92672">-</span> vec_len) <span style="color:#f92672">:</span> (j <span style="color:#f92672">+=</span> vec_len) {
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..N) <span style="color:#f92672">|</span>k<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">const</span> u<span style="color:#f92672">:</span> @Vector(vec_len, <span style="color:#66d9ef">f64</span>) <span style="color:#f92672">=</span> B[k][j..][<span style="color:#ae81ff">0</span>..vec_len].<span style="color:#f92672">*</span>;
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">const</span> y<span style="color:#f92672">:</span> @Vector(vec_len, <span style="color:#66d9ef">f64</span>) <span style="color:#f92672">=</span> C_row.<span style="color:#f92672">*</span>[j..][<span style="color:#ae81ff">0</span>..vec_len].<span style="color:#f92672">*</span>;
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">const</span> w<span style="color:#f92672">:</span> @Vector(vec_len, <span style="color:#66d9ef">f64</span>) <span style="color:#f92672">=</span> @splat(vec_len, A_row.<span style="color:#f92672">*</span>[k]);
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">const</span> slice<span style="color:#f92672">:</span> [vec_len]<span style="color:#66d9ef">f64</span> <span style="color:#f92672">=</span> (u <span style="color:#f92672">*</span> w) <span style="color:#f92672">+</span> y;
</span></span><span style="display:flex;"><span>                @memcpy(C_row.<span style="color:#f92672">*</span>[j .. j <span style="color:#f92672">+</span> vec_len], <span style="color:#f92672">&amp;</span>slice);
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> (j <span style="color:#f92672">&lt;</span> N) <span style="color:#f92672">:</span> (j <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>) {
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> (<span style="color:#ae81ff">0</span>..N) <span style="color:#f92672">|</span>k<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>                C_row.<span style="color:#f92672">*</span>[j] <span style="color:#f92672">+=</span> A_row.<span style="color:#f92672">*</span>[k] <span style="color:#f92672">*</span> B[k][j];
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Note that I replaced the <code>i</code> loop and decided to loop over the rows of <code>C</code> and <code>A</code> directly. What we are doing is again straightforward.
We just take <code>vec_len = 32</code> elements <code>B[k][j], B[k][j+1], ..., B[k][j + 31]</code>, multiply them by <code>A[i][k]</code> (which is now <code>A_row.*[k]</code>), and then
store it in <code>C[i][j], C[i][j+1], ..., C[i][j + 31]</code> (which is now <code>C_row.*[j], C_row.*[j+1], ..., C_row.*[j+31]</code>). Again, if we have less than 32
elements remaining, we revert back to the standard multiplication algorithm. As always, the <code>perf</code> output is below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>5.233718283e+03 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> Performance counter stats <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;./matrix&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       101,785,707      cache-misses              <span style="color:#75715e">#   63.052 % of all cache refs</span>
</span></span><span style="display:flex;"><span>       161,432,535      cache-references
</span></span><span style="display:flex;"><span>    16,377,067,907      instructions              <span style="color:#75715e">#    1.15  insn per cycle</span>
</span></span><span style="display:flex;"><span>    14,227,983,666      cycles
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       5.462324961 seconds time elapsed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       5.457798000 seconds user
</span></span><span style="display:flex;"><span>       0.004001000 seconds sys
</span></span></code></pre></div><p>Compared to our previous output, this isn&rsquo;t great. However, still a significant improvement from our naive and only-transpose matrix
multiplication functions. The main issue here is the <code>k</code>-loop: though we are leveraging <code>Vector</code> to use nearby data, we are still missing
the cache (and in fact our proportion of cache-misses in relation to cache-references is similar to the only-transpose matrix function).
Still we are not using up extra memory, which is a good bonus. However, there is one more optimization we can do.</p>
<h2 id="optimization-4-compilation-arguments">Optimization 4: Compilation Arguments<a hidden class="anchor" aria-hidden="true" href="#optimization-4-compilation-arguments">#</a></h2>
<p>By default, Zig is in the Debug build mode, which means that it enables all runtime safety checks with no optimizations. However, we can
change this build mode by running <code>zig build-exe -O ReleaseFast ./matrix</code> (which builds without runtime-safety checks and optimizes for speed).
Now running <code>perf</code> on the unrolled SIMD loop, we get the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>1.636596222e+03 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> Performance counter stats <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;./matrix&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       116,986,587      cache-misses              <span style="color:#75715e">#   70.848 % of all cache refs</span>
</span></span><span style="display:flex;"><span>       165,123,281      cache-references
</span></span><span style="display:flex;"><span>     1,133,618,990      instructions              <span style="color:#75715e">#    0.29  insn per cycle</span>
</span></span><span style="display:flex;"><span>     3,970,347,473      cycles
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       1.652725296 seconds time elapsed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>       1.640017000 seconds user
</span></span><span style="display:flex;"><span>       0.012000000 seconds sys
</span></span></code></pre></div><p>With quite literally zero coding effort or thinking at all, we have beaten our previous record! To be fair, if you run <code>zig build-exe -O ReleaseFast ./matrix</code>
using the <code>transposeSimdMatrixMultiply</code> function, you will find that it is still faster with optimizations as well. However, considering we were trying to avoid
transposes and put in minimal effort, I would say this level of optimization is pretty good. Another thing I should note is that a significant proportion of the
time is taken with the builtin <code>@memcpy</code> function. Running <code>perf record -a ./matrix</code> in Debug mode and then looking at <code>perf report</code> gives me this output:</p>
<pre tabindex="0"><code> 81.08%  matrix           matrix                               [.] matrix.unrollSimdMatrixMultiply__anon_3611                            
   9.86%  matrix           matrix                               [.] memcpy                                                                
   1.46%  swapper          [unknown]                            [k] 0xffffffffb372cdaf                                                    
   1.42%  matrix           matrix                               [.] rand.Xoshiro256.fill
... rest of output omitted
</code></pre><p>I also tried using <code>std.mem.copy</code>, but it was actually worse than the builtin function. However, the reason I used <code>@memcpy</code> was because there doesn&rsquo;t seem to be another choice.
There was no &ldquo;store&rdquo; function I could use. If we were in C, we could have just used something like <code>_mm256_store_pd</code> to store the data into <code>C</code>, but the <code>Vector</code> datatype
does not seem to have anything like that. However, I think the <code>Vector</code> interface is still being worked on, so it&rsquo;s possible this will be ironed out in later versions.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>In the resources section below, I provided some links to some good material I found on matrix multiplication and how to optimize it. If you look at them, you&rsquo;ll find
that it can get quite involved really quickly. However, most of the optimizations are related to improving cache hits. Furthermore, what I did in this article is by no means
the limit, and especially not with Zig. I&rsquo;m sure if you dug into the <code>@memcpy</code> function, figured out how to use SIMD intrinsics within Zig, or used some of the other builtin
functions (like <code>@preFetch</code>, which sounds quite useful in this case), you can further optimize what I wrote. Plus, I&rsquo;m a complete beginner to Zig, so I&rsquo;m pretty sure a lot of
what I wrote was suboptimal to some degree. Nevertheless, I&rsquo;m quite optimistic to see a performance-oriented language like this, and being able to optimize a complex problem
like this very quickly is extremely promising. The full code is available <a href="https://gist.github.com/svaniksharma/9ad2fa148254ac74b02940326090b18d">here</a>.</p>
<h2 id="resources">Resources<a hidden class="anchor" aria-hidden="true" href="#resources">#</a></h2>
<ul>
<li><a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">What Every Programmer Should Know About Memory</a></li>
<li><a href="https://en.algorithmica.org/hpc/algorithms/matmul/">Matrix Multiplication</a></li>
<li><a href="https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm">Matrix multiplication algorithm</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://svaniksharma.github.io/">Svanik Sharma&#39;s Website</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
